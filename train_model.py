# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lFIzBsY4f0bPuTtlxRbgjYI4anU1gZej
"""

# Imports

import pandas as pd
import numpy as np
import joblib

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import xgboost as xgb

# Load & Sanitize Data

df = pd.read_csv("recruitment_data.csv")

# Remove any leading/trailing whitespace from column names automatically
df.columns = df.columns.str.strip()

# Based on Kaggle schema: HiringDecision is the target
X = df.drop("HiringDecision", axis=1)
y = df["HiringDecision"]


# Define Features (Kaggle Schema)


numeric_features = [
    "Age",
    "ExperienceYears",
    "PreviousCompanies",
    "DistanceFromCompany",
    "InterviewScore",
    "SkillScore",
    "PersonalityScore"
]

categorical_features = [
    "Gender",
    "EducationLevel",
    "RecruitmentStrategy"
]


#  Split Data (Before Preprocessing)


X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Preprocessing Pipeline

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ]
)


X_train = preprocessor.fit_transform(X_train_raw)
X_test = preprocessor.transform(X_test_raw)

# Save preprocessor for production use
joblib.dump(preprocessor, "preprocessor.pkl")

#  Neural Network Model

nn_model = Sequential([
    Dense(64, activation="relu", input_shape=(X_train.shape[1],)),
    Dropout(0.2),  # Prevents overfitting
    Dense(32, activation="relu"),
    Dense(1, activation="sigmoid")
])

nn_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

print("\nTraining Neural Network...")
history=nn_model.fit(
    X_train, y_train,
    epochs=60,
    batch_size=32,
    validation_split=0.1,
    callbacks=[early_stop],
    verbose=0
)


#  XGBoost Model

print("Training XGBoost...")
xgb_model = xgb.XGBClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=4,
    random_state=42,
    eval_metric='logloss'
)

xgb_model.fit(X_train, y_train)


# Evaluation

def show_results(y_real, y_pred, name):
    print(f"\n===== {name.upper()} RESULTS =====")
    print(f"Accuracy: {accuracy_score(y_real, y_pred):.4f}")
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_real, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_real, y_pred))

# NN Predictions
nn_preds = (nn_model.predict(X_test) > 0.5).astype(int)
show_results(y_test, nn_preds, "Neural Network")

# XGBoost Predictions
xgb_preds = xgb_model.predict(X_test)
show_results(y_test, xgb_preds, "XGBoost")


#Export Models

nn_model.save("nn_hiring_model.h5")
joblib.dump(xgb_model, "xgb_hiring_model.pkl")
print("\nModels and Preprocessor saved successfully.")

"""**Accuracy Comparison Bar Chart (NN vs XGBoost)**"""

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt


nn_acc = accuracy_score(y_test, nn_preds)
xgb_acc = accuracy_score(y_test, xgb_preds)

models = ["Neural Network", "XGBoost"]
accuracies = [nn_acc, xgb_acc]

plt.figure()
plt.bar(models, accuracies)
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("Model Accuracy Comparison")
plt.show()

"""**Neural Network Training vs Validation Loss**"""

plt.figure()
plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Neural Network Training vs Validation Loss")
plt.legend()
plt.show()

"""**XGBoost Feature Importance**"""

xgb.plot_importance(xgb_model, max_num_features=10)
plt.title("Top 10 Feature Importances (XGBoost)")
plt.show()

"""**Confusion Matrix Heatmap**"""

from sklearn.metrics import ConfusionMatrixDisplay

# Neural Network Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, nn_preds)
plt.title("Neural Network Confusion Matrix")
plt.show()

# XGBoost Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, xgb_preds)
plt.title("XGBoost Confusion Matrix")
plt.show()